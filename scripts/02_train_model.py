"""
Treinamento de Modelo CNN para Classifica√ß√£o de Ra√ßas de Cachorros
Trabalho Final de CSN - Machine Learning
Autor: Luis
Data: 26/11/2025

Este script implementa transfer learning usando EfficientNetB0 pr√©-treinado no ImageNet.
Inclui data augmentation, callbacks para early stopping e salvamento do melhor modelo.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix
import json
from datetime import datetime

# Configura√ß√µes de GPU (se dispon√≠vel)
print("=" * 80)
print("CONFIGURA√á√ÉO DO AMBIENTE")
print("=" * 80)
print(f"\nüîß TensorFlow vers√£o: {tf.__version__}")
print(f"üéÆ GPUs dispon√≠veis: {len(tf.config.list_physical_devices('GPU'))}")

if tf.config.list_physical_devices('GPU'):
    print("‚úì GPU detectada! Treinamento ser√° acelerado.")
    # Configurar para crescimento de mem√≥ria din√¢mico
    for gpu in tf.config.list_physical_devices('GPU'):
        tf.config.experimental.set_memory_growth(gpu, True)
else:
    print("‚ö† Nenhuma GPU detectada. Treinamento ser√° em CPU.")

# Seed para reprodutibilidade
np.random.seed(42)
tf.random.set_seed(42)

# Diret√≥rios
BASE_DIR = Path(__file__).parent / "archive"
CSV_PATH = BASE_DIR / "dogs.csv"
MODEL_DIR = Path(__file__).parent / "models"
MODEL_DIR.mkdir(exist_ok=True)

# Hiperpar√¢metros
IMG_SIZE = (224, 224)  # Tamanho de entrada do EfficientNetB0
BATCH_SIZE = 32
EPOCHS = 30  # Suficiente para 12 classes
LEARNING_RATE = 0.00005  # Bem menor para treinar todas as camadas

def load_data():
    """Carrega o dataset e prepara os caminhos"""
    print("\n" + "=" * 80)
    print("CARREGAMENTO DOS DADOS")
    print("=" * 80)
    
    df = pd.read_csv(CSV_PATH)
    
    # Adiciona o caminho completo
    df['full_path'] = df['filepaths'].apply(lambda x: str(BASE_DIR / x))
    
    print(f"\nüìä Total de imagens: {len(df)}")
    print(f"üêï Total de ra√ßas: {df['labels'].nunique()}")
    
    # Seleciona 12 ra√ßas visualmente MUITO distintas para melhor acur√°cia
    selected_breeds = [
        'Siberian Husky',    # Olhos azuis, pelagem cinza/branca
        'Pug',               # Focinho achatado, pequeno
        'Dalmation',         # Manchas pretas √∫nicas
        'German Sheperd',    # Pastor alem√£o cl√°ssico
        'Golden Retriever',  # Dourado, pelo longo
        'Beagle',            # Tricolor, orelhas ca√≠das
        'Bulldog',           # Corpo atarracado, focinho achatado
        'Chihuahua',         # Muito pequeno
        'Doberman',          # Preto/marrom, orelhas pontiagudas
        'Great Dane',        # Gigante
        'Rottweiler',        # Preto com marcas marrom
        'Chow'               # L√≠ngua azul, pelagem densa
    ]
    
    # Separar por conjunto
    train_df = df[df['data set'] == 'train'].copy()
    valid_df = df[df['data set'] == 'valid'].copy()
    test_df = df[df['data set'] == 'test'].copy()
    
    # Filtra apenas as ra√ßas selecionadas
    print(f"\n‚ö†Ô∏è  Filtrando para {len(selected_breeds)} ra√ßas visualmente distintas...")
    print(f"   Ra√ßas selecionadas: {', '.join(selected_breeds[:4])}...")
    
    train_df = train_df[train_df['labels'].isin(selected_breeds)].copy()
    valid_df = valid_df[valid_df['labels'].isin(selected_breeds)].copy()
    test_df = test_df[test_df['labels'].isin(selected_breeds)].copy()
    
    common_breeds = set(train_df['labels'].unique())
    
    print(f"\nüì¶ Divis√£o do dataset:")
    print(f"   Treino:     {len(train_df):5d} imagens ({len(train_df)/len(df)*100:5.1f}%)")
    print(f"   Valida√ß√£o:  {len(valid_df):5d} imagens ({len(valid_df)/len(df)*100:5.1f}%)")
    print(f"   Teste:      {len(test_df):5d} imagens ({len(test_df)/len(df)*100:5.1f}%)")
    
    return train_df, valid_df, test_df, len(common_breeds)

def create_data_generators(train_df, valid_df, test_df):
    """
    Cria geradores de dados com data augmentation.
    
    Data Augmentation √© crucial para:
    - Reduzir overfitting
    - Aumentar a robustez do modelo a varia√ß√µes
    - Simular diferentes condi√ß√µes de captura
    """
    print("\n" + "=" * 80)
    print("CRIA√á√ÉO DOS GERADORES DE DADOS")
    print("=" * 80)
    
    # Generator para treino COM data augmentation
    train_datagen = ImageDataGenerator(
        rescale=1./255,              # Normaliza√ß√£o [0,1]
        rotation_range=20,           # Rota√ß√£o aleat√≥ria ¬±20¬∞
        width_shift_range=0.2,       # Deslocamento horizontal ¬±20%
        height_shift_range=0.2,      # Deslocamento vertical ¬±20%
        shear_range=0.2,             # Cisalhamento
        zoom_range=0.2,              # Zoom aleat√≥rio ¬±20%
        horizontal_flip=True,        # Espelhamento horizontal
        fill_mode='nearest'          # Preenchimento de pixels vazios
    )
    
    # Generator para valida√ß√£o e teste SEM augmentation (apenas normaliza√ß√£o)
    val_test_datagen = ImageDataGenerator(rescale=1./255)
    
    print("\nüîÑ Data Augmentation configurado:")
    print("   ‚Ä¢ Rota√ß√£o: ¬±20¬∞")
    print("   ‚Ä¢ Deslocamento: ¬±20%")
    print("   ‚Ä¢ Zoom: ¬±20%")
    print("   ‚Ä¢ Espelhamento horizontal")
    print("   ‚Ä¢ Cisalhamento: 0.2")
    
    # Cria os geradores
    train_generator = train_datagen.flow_from_dataframe(
        dataframe=train_df,
        x_col='full_path',
        y_col='labels',
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=True,
        seed=42
    )
    
    valid_generator = val_test_datagen.flow_from_dataframe(
        dataframe=valid_df,
        x_col='full_path',
        y_col='labels',
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=False
    )
    
    test_generator = val_test_datagen.flow_from_dataframe(
        dataframe=test_df,
        x_col='full_path',
        y_col='labels',
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=False
    )
    
    print(f"\n‚úì Geradores criados:")
    print(f"   Treino:    {train_generator.samples} amostras, {len(train_generator)} batches")
    print(f"   Valida√ß√£o: {valid_generator.samples} amostras, {len(valid_generator)} batches")
    print(f"   Teste:     {test_generator.samples} amostras, {len(test_generator)} batches")
    
    # Salva o mapeamento de classes
    class_indices = train_generator.class_indices
    with open(MODEL_DIR / 'class_indices.json', 'w') as f:
        json.dump(class_indices, f, indent=2)
    print(f"\nüíæ Mapeamento de classes salvo em: {MODEL_DIR / 'class_indices.json'}")
    
    return train_generator, valid_generator, test_generator, class_indices

def build_model(num_classes):
    """
    Constr√≥i modelo usando Transfer Learning com EfficientNetB0.
    
    Transfer Learning:
    - Usa conhecimento aprendido no ImageNet (1.4M imagens, 1000 classes)
    - Congela as camadas convolucionais (extra√ß√£o de features)
    - Treina apenas as camadas finais de classifica√ß√£o
    - Muito mais eficiente que treinar do zero
    
    EfficientNetB0:
    - Arquitetura moderna, balanceando profundidade, largura e resolu√ß√£o
    - Compound scaling method
    - Excelente trade-off entre acur√°cia e efici√™ncia
    """
    print("\n" + "=" * 80)
    print("CONSTRU√á√ÉO DO MODELO")
    print("=" * 80)
    
    # Carrega modelo base pr√©-treinado
    base_model = EfficientNetB0(
        include_top=False,           # Remove camada de classifica√ß√£o original
        weights='imagenet',          # Usa pesos pr√©-treinados
        input_shape=(*IMG_SIZE, 3)
    )
    
    # Descongela TODAS as camadas para treinar o modelo completo
    base_model.trainable = True
    
    print(f"\nüèóÔ∏è  Arquitetura base: EfficientNetB0")
    print(f"   Par√¢metros totais: {base_model.count_params():,}")
    print(f"   Camadas congeladas: {len(base_model.layers)}")
    
    # Constr√≥i o modelo completo
    inputs = keras.Input(shape=(*IMG_SIZE, 3))
    
    # Modelo base
    x = base_model(inputs, training=False)
    
    # Global Average Pooling (reduz espacialidade)
    x = layers.GlobalAveragePooling2D()(x)
    
    # Camadas densas de classifica√ß√£o
    x = layers.Dense(512, activation='relu')(x)
    x = layers.Dropout(0.5)(x)  # Regulariza√ß√£o
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    
    # Camada de sa√≠da
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    
    model = keras.Model(inputs, outputs)
    
    # Compila o modelo
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='categorical_crossentropy',
        metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=5, name='top5_accuracy')]
    )
    
    print(f"\nüìä Modelo final:")
    print(f"   Par√¢metros trein√°veis: {sum([np.prod(v.shape) for v in model.trainable_weights]):,}")
    print(f"   Par√¢metros n√£o-trein√°veis: {sum([np.prod(v.shape) for v in model.non_trainable_weights]):,}")
    print(f"   Classes de sa√≠da: {num_classes}")
    
    return model

def create_callbacks(model_name='dog_classifier'):
    """
    Cria callbacks para treinamento.
    
    Callbacks:
    - EarlyStopping: Para quando n√£o h√° melhoria (evita overfitting)
    - ModelCheckpoint: Salva o melhor modelo
    - ReduceLROnPlateau: Reduz learning rate quando estagna
    """
    print("\n" + "=" * 80)
    print("CONFIGURA√á√ÉO DE CALLBACKS")
    print("=" * 80)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_path = MODEL_DIR / f"{model_name}_{timestamp}.keras"
    
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=10,  # Ajustado para deadline
            restore_best_weights=True,
            verbose=1
        ),
        ModelCheckpoint(
            filepath=str(model_path),
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=4,  # Ajustado
            min_lr=1e-7,
            verbose=1
        )
    ]
    
    print("\n‚úì Callbacks configurados:")
    print("   ‚Ä¢ Early Stopping (patience=20)")
    print("   ‚Ä¢ Model Checkpoint")
    print("   ‚Ä¢ Reduce LR on Plateau (factor=0.5, patience=8)")
    print(f"\nüíæ Modelo ser√° salvo em: {model_path}")
    
    return callbacks, model_path

def plot_training_history(history, save_path='training_history.png'):
    """Plota curvas de treinamento"""
    print("\nüìà Gerando gr√°ficos de treinamento...")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Acur√°cia
    ax = axes[0, 0]
    ax.plot(history.history['accuracy'], label='Treino', linewidth=2)
    ax.plot(history.history['val_accuracy'], label='Valida√ß√£o', linewidth=2)
    ax.set_title('Acur√°cia do Modelo', fontsize=14, fontweight='bold')
    ax.set_xlabel('√âpoca')
    ax.set_ylabel('Acur√°cia')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Loss
    ax = axes[0, 1]
    ax.plot(history.history['loss'], label='Treino', linewidth=2)
    ax.plot(history.history['val_loss'], label='Valida√ß√£o', linewidth=2)
    ax.set_title('Perda (Loss) do Modelo', fontsize=14, fontweight='bold')
    ax.set_xlabel('√âpoca')
    ax.set_ylabel('Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Top-5 Acur√°cia
    ax = axes[1, 0]
    ax.plot(history.history['top5_accuracy'], label='Treino', linewidth=2)
    ax.plot(history.history['val_top5_accuracy'], label='Valida√ß√£o', linewidth=2)
    ax.set_title('Top-5 Acur√°cia', fontsize=14, fontweight='bold')
    ax.set_xlabel('√âpoca')
    ax.set_ylabel('Top-5 Acur√°cia')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Compara√ß√£o final
    ax = axes[1, 1]
    final_metrics = {
        'Treino': [history.history['accuracy'][-1], history.history['top5_accuracy'][-1]],
        'Valida√ß√£o': [history.history['val_accuracy'][-1], history.history['val_top5_accuracy'][-1]]
    }
    x = np.arange(2)
    width = 0.35
    ax.bar(x - width/2, final_metrics['Treino'], width, label='Treino', color='#3498db')
    ax.bar(x + width/2, final_metrics['Valida√ß√£o'], width, label='Valida√ß√£o', color='#2ecc71')
    ax.set_title('M√©tricas Finais', fontsize=14, fontweight='bold')
    ax.set_ylabel('Score')
    ax.set_xticks(x)
    ax.set_xticklabels(['Acur√°cia', 'Top-5 Acur√°cia'])
    ax.legend()
    ax.set_ylim([0, 1])
    ax.grid(True, alpha=0.3, axis='y')
    
    # Adiciona valores nas barras
    for i, (k, v) in enumerate(final_metrics.items()):
        for j, val in enumerate(v):
            x_pos = j + (i - 0.5) * width
            ax.text(x_pos, val + 0.02, f'{val:.3f}', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"   ‚úì Salvo: {save_path}")
    plt.show()

def main():
    """Fun√ß√£o principal de treinamento"""
    print("\n" + "=" * 80)
    print("üêï TREINAMENTO DO CLASSIFICADOR DE RA√áAS DE CACHORROS üêï")
    print("=" * 80)
    
    # 1. Carrega dados
    train_df, valid_df, test_df, num_classes = load_data()
    
    # 2. Cria geradores
    train_gen, valid_gen, test_gen, class_indices = create_data_generators(train_df, valid_df, test_df)
    
    # 3. Constr√≥i modelo
    model = build_model(num_classes)
    
    # 4. Configura callbacks
    callbacks, model_path = create_callbacks()
    
    # 5. Treina modelo
    print("\n" + "=" * 80)
    print("üöÄ INICIANDO TREINAMENTO")
    print("=" * 80)
    print(f"\n‚öôÔ∏è  Configura√ß√£o:")
    print(f"   √âpocas m√°ximas: {EPOCHS}")
    print(f"   Batch size: {BATCH_SIZE}")
    print(f"   Learning rate: {LEARNING_RATE}")
    print(f"   Imagem size: {IMG_SIZE}")
    print("\n")
    
    history = model.fit(
        train_gen,
        validation_data=valid_gen,
        epochs=EPOCHS,
        callbacks=callbacks,
        verbose=1
    )
    
    # 6. Avalia√ß√£o final no conjunto de teste
    print("\n" + "=" * 80)
    print("üìä AVALIA√á√ÉO NO CONJUNTO DE TESTE")
    print("=" * 80)
    
    test_loss, test_acc, test_top5 = model.evaluate(test_gen, verbose=1)
    
    print(f"\n‚úì Resultados finais:")
    print(f"   Loss:           {test_loss:.4f}")
    print(f"   Acur√°cia:       {test_acc:.4f} ({test_acc*100:.2f}%)")
    print(f"   Top-5 Acur√°cia: {test_top5:.4f} ({test_top5*100:.2f}%)")
    
    # 7. Salva m√©tricas
    results = {
        'test_loss': float(test_loss),
        'test_accuracy': float(test_acc),
        'test_top5_accuracy': float(test_top5),
        'num_classes': num_classes,
        'training_samples': len(train_df),
        'validation_samples': len(valid_df),
        'test_samples': len(test_df),
        'epochs_trained': len(history.history['loss']),
        'timestamp': datetime.now().isoformat()
    }
    
    with open(MODEL_DIR / 'training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nüíæ Resultados salvos em: {MODEL_DIR / 'training_results.json'}")
    
    # 8. Plota hist√≥rico
    plot_training_history(history)
    
    print("\n" + "=" * 80)
    print("‚úì TREINAMENTO CONCLU√çDO COM SUCESSO!")
    print("=" * 80)
    print(f"\nüìÅ Arquivos gerados:")
    print(f"   ‚Ä¢ {model_path}")
    print(f"   ‚Ä¢ {MODEL_DIR / 'class_indices.json'}")
    print(f"   ‚Ä¢ {MODEL_DIR / 'training_results.json'}")
    print(f"   ‚Ä¢ training_history.png")
    print("\n")

if __name__ == "__main__":
    main()
